{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bb5e692",
   "metadata": {},
   "source": [
    "## Latent decomposition and recombination for controlled hallucination\n",
    "Parts of this code were inspired by [goodboychan's VAE implementation](https://colab.research.google.com/github/goodboychan/goodboychan.github.io/blob/main/_notebooks/2021-09-14-03-Variational-AutoEncoder-Celeb-A.ipynb), and [TensorFlow's CycleGAN implementation](https://www.tensorflow.org/tutorials/generative/cyclegan)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1ab14c5",
   "metadata": {},
   "source": [
    "ssh -i \"C:\\Users\\Thijs\\OneDrive\\University\\Year 3\\Thesis\\GPU access\\privateKey.pem\" u529937@aurometalsaurus.uvt.nl\n",
    "\n",
    "srun --nodes=1 --pty /bin/bash -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e544e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/usr/lib/cuda\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp \n",
    "distributor = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b093aa1c",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd70e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants used throughout the script\n",
    "INPUT_SHAPE = (256, 256, 3)\n",
    "FILTERS = [64,128,256,512,512]\n",
    "BATCH_SIZE = 16\n",
    "DIST_BATCH_SIZE = BATCH_SIZE//max(len(tf.config.list_physical_devices(\"GPU\")), 1)\n",
    "LATENT_DIM = 254\n",
    "PATH = \"../../data/cats2dogs/PetImages\"\n",
    "\n",
    "# Define a generator to yield images and its path as a label\n",
    "def gen(path, train=True):\n",
    "    for file in os.listdir(path)[slice(0, -1000) if train else slice(-1000, None)]:\n",
    "        # Get file\n",
    "        image = tf.io.read_file(os.path.join(path, file))\n",
    "        image = tf.io.decode_image(image, dtype=tf.float32)\n",
    "        # Crop to largest square\n",
    "        h, w = image.shape[:-1]\n",
    "        d = tf.minimum(h, w)\n",
    "        image = tf.image.crop_to_bounding_box(image, (h-d)//2, (w-d)//2, d, d)\n",
    "        # Prevent grayscale\n",
    "        if image.shape[-1]!=3:\n",
    "           continue\n",
    "        yield image\n",
    "\n",
    "# Define a function to randomly modify images\n",
    "def random_jitter(image):\n",
    "  # Expand and crop\n",
    "  image = tf.image.resize(image, (286,286))\n",
    "  image = tf.image.random_crop(image, size=INPUT_SHAPE)\n",
    "  # Random mirroring\n",
    "  image = tf.image.random_flip_left_right(image)\n",
    "  return image\n",
    "\n",
    "# Load training data of class A (note that corrupt files were removed)\n",
    "train_A = tf.data.Dataset.from_generator(\n",
    "    gen,\n",
    "    args=[os.path.join(PATH, \"Cat\")],\n",
    "    output_signature=tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32),\n",
    ").cache().repeat().map(random_jitter).shuffle(3, reshuffle_each_iteration=True).batch(BATCH_SIZE)\n",
    "# Load training data of class B\n",
    "train_B = tf.data.Dataset.from_generator(\n",
    "    gen,\n",
    "    args=[os.path.join(PATH, \"Dog\")],\n",
    "    output_signature=tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32),\n",
    ").cache().repeat().map(random_jitter).shuffle(3, reshuffle_each_iteration=True).batch(BATCH_SIZE)\n",
    "# Zip the training data\n",
    "train = tf.data.Dataset.zip((train_A, train_B))\n",
    "train = distributor.experimental_distribute_dataset(train)\n",
    "\n",
    "# Load test data of class A\n",
    "test_A = tf.data.Dataset.from_generator(\n",
    "    gen,\n",
    "    args=[os.path.join(PATH, \"Cat\"), False],\n",
    "    output_signature=(tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32)),\n",
    ").map(random_jitter).batch(BATCH_SIZE, drop_remainder=True)\n",
    "# Load test data fo class B\n",
    "test_B = tf.data.Dataset.from_generator(\n",
    "    gen,\n",
    "    args=[os.path.join(PATH, \"Dog\"), False],\n",
    "    output_signature=tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32),\n",
    ").map(random_jitter).batch(BATCH_SIZE, drop_remainder=True)\n",
    "# Zip the test data\n",
    "test = tf.data.Dataset.zip((test_A, test_B))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10322b1c",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f75beaa5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define a function that creates a Sequential model consisting of convolutions\n",
    "def downsampler(nfilters, name=None, strides=(1,1), size=(12,12)):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dropout(rate=.4),\n",
    "        tf.keras.layers.Conv2D(filters=nfilters, kernel_size=size, padding=\"same\", strides=strides),\n",
    "        tf.keras.layers.GroupNormalization(groups=nfilters),\n",
    "        tf.keras.layers.LeakyReLU()\n",
    "    ], name=name)\n",
    "\n",
    "# Define a function that creates a Sequential model consisting of transposed convolutions\n",
    "def upsampler(nfilters, name=None, strides=(2,2), size=(12,12)):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dropout(rate=.4),\n",
    "        tf.keras.layers.Conv2DTranspose(filters=nfilters, kernel_size=size, strides=strides, padding=\"same\"),\n",
    "        tf.keras.layers.GroupNormalization(groups=nfilters),\n",
    "        tf.keras.layers.LeakyReLU()\n",
    "    ], name=name)\n",
    "\n",
    "# Define a function to return an inception module (defaults to downsampling unless `resize_shape` is given)\n",
    "def inceptionv2(input_shape, nfilters, strides=(1,1), block=downsampler, name=None):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, batch_size=BATCH_SIZE)\n",
    "    # Separate into brach 1\n",
    "    branch1 = block(nfilters=nfilters//4, size=(1,1), strides=strides)(inputs)\n",
    "    # Separate into brach 2\n",
    "    branch2 = block(nfilters=nfilters//4, size=(1,1), strides=(1,1))(inputs)\n",
    "    branch2 = block(nfilters=nfilters//4, size=(6,6), strides=strides)(branch2)\n",
    "    # Separate into brach 3\n",
    "    branch3 = block(nfilters=nfilters//4, size=(1,1), strides=(1,1))(inputs)\n",
    "    branch3 = block(nfilters=nfilters//4, size=(12,12), strides=strides)(branch3)\n",
    "    # Separate into brach 4\n",
    "    if block==upsampler:\n",
    "        branch4 = tf.keras.layers.UpSampling2D(size=(3,3))(inputs)\n",
    "        cropping = tuple(map(lambda i, j: (i-j)//2, branch4.shape[1:-1], input_shape[:-1]))\n",
    "        branch4 = tf.keras.layers.Cropping2D(cropping)(branch4)\n",
    "    else:\n",
    "        branch4 = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(1,1), padding=\"same\")(inputs)\n",
    "    branch4 = block(nfilters=nfilters//4, size=(1,1), strides=strides)(branch4)\n",
    "    # Concatenate into desired dimensions\n",
    "    outputs = tf.keras.layers.Concatenate()([branch1, branch2, branch3, branch4])\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "# Define a function to create a prior distribution used to condition the latent distribution\n",
    "def get_prior():\n",
    "    return tfp.distributions.MultivariateNormalDiag(\n",
    "        loc=tf.Variable(\n",
    "            tf.random.normal(shape=(LATENT_DIM,)), \n",
    "            trainable=True, \n",
    "            name=\"mu\"\n",
    "        ),\n",
    "        scale_diag=tfp.util.TransformedVariable(\n",
    "            tf.Variable(tf.ones(shape=(LATENT_DIM,))), \n",
    "            bijector=tfp.bijectors.Softplus(), \n",
    "            name=\"sigma\", \n",
    "            trainable=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Define a function to build the encoders\n",
    "def build_encoder(name=\"Encoder\"):\n",
    "    # Get input\n",
    "    inputs = tf.keras.layers.Input(shape=INPUT_SHAPE, batch_size=BATCH_SIZE, name=\"Input\")\n",
    "    x = inputs\n",
    "    # Create some downsampling modules\n",
    "    for i, nfilters in enumerate(FILTERS):\n",
    "        # Create identity with specified number of filters\n",
    "        x_ = downsampler(nfilters, size=(1,1), name=f\"Downsampler_{i*2}\")(x)\n",
    "        # Pass through inception modules\n",
    "        x = downsampler(nfilters=nfilters, size=(3,3), name=f\"Downsampler_{i*2+1}\")(x)\n",
    "        x = inceptionv2(input_shape=x.shape[1:], nfilters=nfilters, name=f\"Inception_{i}\")(x)\n",
    "        # Add identity and skipp layers\n",
    "        x = tf.keras.layers.Add(name=f\"SumSkips_{i}\")([x_, x])\n",
    "        # Reduce size\n",
    "        x = tf.keras.layers.AveragePooling2D(pool_size=(2,2), name=f\"AveragePooling_{i}\")(x)\n",
    "    # Resize to enable the tfp layer\n",
    "    x = tf.keras.layers.Reshape((-1,1), name=\"Reshape\")(x)\n",
    "    cropping = (x.shape[1] - tfp.layers.MultivariateNormalTriL.params_size(LATENT_DIM))/2\n",
    "    cropping = (int(cropping-cropping%1), int(cropping+cropping%1))\n",
    "    x = tf.keras.layers.Cropping1D(cropping=cropping, name=\"Cropping\")(x)\n",
    "    x = tf.keras.layers.Flatten(name=\"Flatten\")(x)\n",
    "    # Process in a distribution layer\n",
    "    latent_dist = tfp.layers.MultivariateNormalTriL(\n",
    "        LATENT_DIM, \n",
    "        name=\"LatentDistribution\",\n",
    "        activity_regularizer=tfp.layers.KLDivergenceRegularizer(get_prior(), use_exact_kl=True),\n",
    "    )(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=latent_dist, name=name)\n",
    "\n",
    "# Define a function to build the decoder\n",
    "def build_decoder():\n",
    "    # Get input\n",
    "    style = tf.keras.layers.Input(shape=LATENT_DIM, batch_size=BATCH_SIZE, name=\"StyleInput\")\n",
    "    content = tf.keras.layers.Input(shape=LATENT_DIM, batch_size=BATCH_SIZE, name=\"ContentInput\")\n",
    "    # Concatenate latent representations\n",
    "    x = tf.keras.layers.Concatenate(name=\"Concatenate\")([style, content])\n",
    "    # Resize to the last downsampling module's shape\n",
    "    x = tf.keras.layers.Dense(8*8*512, activation=\"relu\", name=\"Dense\")(x)\n",
    "    x = tf.keras.layers.Reshape((8,8,512), name=\"Reshape\")(x)\n",
    "    # Revert the downsampling\n",
    "    for i, nfilters in enumerate(reversed(FILTERS)):\n",
    "        # Make skip connection\n",
    "        x_ = upsampler(nfilters, size=(1,1), name=f\"Upsampler_{i*2}\")(x)\n",
    "        # Pass input through inception module\n",
    "        x = upsampler(nfilters=nfilters, size=(3,3), strides=(2,2), name=f\"Upsampler_{i*2+1}\")(x)\n",
    "        x = inceptionv2(input_shape=x.shape[1:], block=upsampler, nfilters=nfilters, strides=(1,1), name=f\"Inception_{i}\")(x)\n",
    "       # Sum skip and inception\n",
    "        x = tf.keras.layers.Add(name=f\"SumSkips_{i}\")([x_, x])\n",
    "    # Reshape to image format\n",
    "    x = upsampler(3, strides=(1,1), name=f\"Upsampler_{i*2+2}\")(x)\n",
    "    # Fit a Bernoulli to leverage probabilistic reconstruction loss\n",
    "    x = tf.keras.layers.Flatten(name=\"Flatten\")(x)\n",
    "    outputs = tfp.layers.IndependentBernoulli(event_shape=INPUT_SHAPE, name=\"Bernoulli\", dtype=tf.float32)(x)\n",
    "    return tf.keras.Model(inputs=[style, content], outputs=outputs, name=\"Decoder\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "255b09f6",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d48840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build within distributed scope\n",
    "with distributor.scope():\n",
    "    # Models\n",
    "    encoder_style = build_encoder(\"StyleEncoder\")\n",
    "    encoder_content = build_encoder(\"ContentEncoder\")\n",
    "    decoder = build_decoder()\n",
    "    # Optimizers\n",
    "    decoder_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    encoder_style_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    encoder_content_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c0a26e2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d35a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "decoder.load_weights(\"./decoder.h5\")\n",
    "encoder_style.load_weights(\"./encoder_style.h5\")\n",
    "encoder_content.load_weights(\"./encoder_content.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a15c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss scaling factors\n",
    "KL_COEF = RECONSTRUCTION_COEF = CYCLE_COEF = 1.\n",
    "\n",
    "# Define a function to get reconstructions, translations, and cycles\n",
    "def training_call(A, B):\n",
    "    # Get latent representations  (stylistic batches are averaged to enforce style to be whatever is common)\n",
    "    style_A = encoder_style(A)\n",
    "    mean_style_A = tf.repeat(tf.reduce_mean(style_A, axis=0, keepdims=True), repeats=DIST_BATCH_SIZE, axis=0)\n",
    "    style_B = encoder_style(B)\n",
    "    mean_style_B = tf.repeat(tf.reduce_mean(style_B, axis=0, keepdims=True), repeats=DIST_BATCH_SIZE, axis=0)\n",
    "    content_A = encoder_content(A)\n",
    "    content_B = encoder_content(B)\n",
    "\n",
    "    # Predict reconstructed images\n",
    "    reconstructed_A = decoder([mean_style_A, content_A])\n",
    "    reconstructed_B = decoder([mean_style_B, content_B])\n",
    "    # Cycle latent representations\n",
    "    style_reconstructed_A = encoder_style(reconstructed_A.mean())\n",
    "    style_reconstructed_B = encoder_style(reconstructed_B.mean())\n",
    "    content_reconstructed_A = encoder_content(reconstructed_A.mean())\n",
    "    content_reconstructed_B = encoder_content(reconstructed_B.mean())\n",
    "\n",
    "    # Translate images\n",
    "    translated_A = decoder([mean_style_B, content_A])\n",
    "    translated_B = decoder([mean_style_A, content_B])\n",
    "    # Cycle latent style representations (using mean because of gradients)\n",
    "    style_translated_A = encoder_style(translated_A.mean())  # i.e., the estimated style of B\n",
    "    style_translated_B = encoder_style(translated_B.mean())  # i.e., the estimated style of A\n",
    "    content_translated_A = encoder_content(translated_A.mean())\n",
    "    content_translated_B = encoder_content(translated_B.mean())\n",
    "\n",
    "    return (\n",
    "        content_A, \n",
    "        content_B, \n",
    "        style_A, \n",
    "        style_B, \n",
    "        reconstructed_A,\n",
    "        reconstructed_B,\n",
    "        style_reconstructed_A,\n",
    "        style_reconstructed_B,\n",
    "        content_reconstructed_A,\n",
    "        content_reconstructed_B,\n",
    "        style_translated_A, \n",
    "        style_translated_B, \n",
    "        content_translated_A, \n",
    "        content_translated_B\n",
    "    )\n",
    "\n",
    "# Define a train step\n",
    "def train_step(data, i):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Split classes\n",
    "        A, B = data\n",
    "\n",
    "        # Get latent representations of the input, reconstructions, and translations\n",
    "        (\n",
    "        content_A, \n",
    "        content_B, \n",
    "        style_A, \n",
    "        style_B, \n",
    "        reconstructed_A,\n",
    "        reconstructed_B,\n",
    "        style_reconstructed_A,\n",
    "        style_reconstructed_B,\n",
    "        content_reconstructed_A,\n",
    "        content_reconstructed_B,\n",
    "        style_translated_A, \n",
    "        style_translated_B, \n",
    "        content_translated_A, \n",
    "        content_translated_B\n",
    "        ) = training_call(A, B)\n",
    "\n",
    "        # Calculate reconstruction loss (to enforce viable output)\n",
    "        reconstruction_loss = -reconstructed_A.log_prob(A)\n",
    "        reconstruction_loss += -reconstructed_B.log_prob(B)\n",
    "        # Calculate cycle loss \n",
    "        cycle_loss = tf.square(content_A-content_reconstructed_A) + tf.square(content_A-content_translated_A)\n",
    "        cycle_loss += tf.square(content_B-content_reconstructed_B) + tf.square(content_B-content_translated_B)\n",
    "        cycle_loss += tf.square(style_A-style_reconstructed_A) + tf.square(style_A-style_translated_B)\n",
    "        cycle_loss += tf.square(style_B-style_reconstructed_B) + tf.square(style_B-style_translated_A)\n",
    "        # Calculate kl-losses\n",
    "        kl_style = tf.add_n(encoder_style.losses)\n",
    "        kl_content = tf.add_n(encoder_content.losses)\n",
    "        # Aggregate losses\n",
    "        loss_decoder = CYCLE_COEF*cycle_loss + tf.reshape((.96**(i/500)+5e-3)*RECONSTRUCTION_COEF*reconstruction_loss, (-1,1))\n",
    "        loss_encoder_style = RECONSTRUCTION_COEF*reconstruction_loss + KL_COEF*kl_style\n",
    "        loss_encoder_content = RECONSTRUCTION_COEF*reconstruction_loss + KL_COEF*kl_content\n",
    "\n",
    "    # Calculate and apply gradients to weights of decoder\n",
    "    grads_decoder = tape.gradient(loss_decoder, decoder.trainable_variables)\n",
    "    decoder_optimizer.apply_gradients(zip(grads_decoder, decoder.trainable_variables))\n",
    "    # Calculate and apply gradients to weights of style encoder\n",
    "    grads_encoder_style = tape.gradient(loss_encoder_style, encoder_style.trainable_variables)\n",
    "    encoder_style_optimizer.apply_gradients(zip(grads_encoder_style, encoder_style.trainable_variables))\n",
    "    # Calculate and apply gradients to weights of style encoder\n",
    "    grads_encoder_content = tape.gradient(loss_encoder_content, encoder_content.trainable_variables)\n",
    "    encoder_content_optimizer.apply_gradients(zip(grads_encoder_content, encoder_content.trainable_variables))\n",
    "\n",
    "    # Return progress\n",
    "    return {\n",
    "        \"reconstruction_loss\":tf.reduce_mean(reconstruction_loss), \n",
    "        \"cycle_loss\":tf.reduce_mean(cycle_loss), \n",
    "        \"kl_regularization\":tf.reduce_mean(kl_style+kl_content)\n",
    "    }\n",
    "\n",
    "# Convert to distributed train step\n",
    "@tf.function\n",
    "def distributed_train_step(data, i):\n",
    "    per_replica_losses = distributor.run(train_step, args=(data, i))\n",
    "    return distributor.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "\n",
    "# Run batches\n",
    "history = {}\n",
    "for j, data in enumerate(train):\n",
    "    i = j+24100\n",
    "    # Stop training when appropriate\n",
    "    if i==1e5:\n",
    "        break\n",
    "    # Train the networks\n",
    "    losses = distributed_train_step(data, tf.constant(i, dtype=tf.float32))\n",
    "    history[i] = {key: float(value) for key, value in losses.items()}\n",
    "    # Periodically print progress\n",
    "    if i%100==0:\n",
    "        print(\n",
    "            f\"Losses at batch {i}:\\n\",\n",
    "            *map(lambda key: f\"{key}: {losses[key]}, \", losses)\n",
    "        )\n",
    "        json.dump(history, open(\"./history.json\", mode=\"w\"))\n",
    "    # Periodically save weights\n",
    "    if (i+1)%100==0:\n",
    "        decoder.save_weights(\"./decoder.h5\")\n",
    "        encoder_style.save_weights(\"./encoder_style.h5\")\n",
    "        encoder_content.save_weights(\"./encoder_content.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78120a3a",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe7438c",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# Define the call function for inference (note the slight changes such as calling `mean`)\n",
    "def inference_call(A, B):\n",
    "    # Get latent style representations  (batches are averaged to enforce style to be whatever is common)\n",
    "    style_A = encoder_style(A).mean()\n",
    "    mean_style_A = tf.repeat(tf.reduce_mean(style_A, axis=0, keepdims=True), repeats=BATCH_SIZE, axis=0)\n",
    "    style_B = encoder_style(B).mean()\n",
    "    mean_style_B = tf.repeat(tf.reduce_mean(style_B, axis=0, keepdims=True), repeats=BATCH_SIZE, axis=0)\n",
    "    # Get latent content representations\n",
    "    content_A = encoder_content(A).mean()\n",
    "    content_B = encoder_content(B).mean()\n",
    "    # Predict reconstructed images\n",
    "    reconstructed_A = decoder([mean_style_A, content_A]).mean()\n",
    "    reconstructed_B = decoder([mean_style_B, content_B]).mean()\n",
    "    # Translate images\n",
    "    translated_A = decoder([mean_style_B, content_A]).mean()\n",
    "    translated_B = decoder([mean_style_A, content_B]).mean()\n",
    "\n",
    "    return reconstructed_A, reconstructed_B, translated_A, translated_B\n",
    "\n",
    "# Load model\n",
    "decoder.load_weights(\"./decoder.h5\")\n",
    "encoder_style.load_weights(\"./encoder_style.h5\")\n",
    "encoder_content.load_weights(\"./encoder_content.h5\")\n",
    "\n",
    "# Show\n",
    "for i, (A, B) in enumerate(test):\n",
    "    # Get images, reconstructions, and translations\n",
    "    reconstructed_A, reconstructed_B, translated_A, translated_B = inference_call(A, B)\n",
    "    # Save images\n",
    "    for j in range(BATCH_SIZE):\n",
    "        tf.io.write_file(f\"./samples/reconstructed_A/{i*BATCH_SIZE+j}.png\", contents=tf.io.encode_png(tf.cast(reconstructed_A[j]*255, tf.uint8)))\n",
    "        tf.io.write_file(f\"./samples/reconstructed_B/{i*BATCH_SIZE+j}.png\", contents=tf.io.encode_png(tf.cast(reconstructed_B[j]*255, tf.uint8)))\n",
    "        tf.io.write_file(f\"./samples/translated_A/{i*BATCH_SIZE+j}.png\", contents=tf.io.encode_png(tf.cast(translated_A[j]*255, tf.uint8)))\n",
    "        tf.io.write_file(f\"./samples/translated_B/{i*BATCH_SIZE+j}.png\", contents=tf.io.encode_png(tf.cast(translated_B[j]*255, tf.uint8)))\n",
    "\n",
    "# Plot models\n",
    "tf.keras.utils.plot_model(model=decoder, dpi=384, to_file=\"./plots/DecoderArchitecture.png\", show_layer_names=False);\n",
    "tf.keras.utils.plot_model(model=encoder_style, dpi=384, to_file=\"./plots/EncoderArchitecture.png\", show_layer_names=False);\n",
    "tf.keras.utils.plot_model(model=encoder_style.layers[4], dpi=384, to_file=\"./plots/InceptionArchitecture.png\", show_layer_names=False);\n",
    "tf.keras.utils.plot_model(model=encoder_style.layers[2], dpi=384, to_file=\"./plots/DownsamplerArchitecture.png\", show_layer_names=False);"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
